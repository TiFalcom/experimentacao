{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/.pyenv/versions/3.12.4/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "project_dir = Path('__main__').resolve().parents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataSet(Dataset):\n",
    "    def __init__(self, features_file, target_file, mapping_file):\n",
    "        self.features_table = pd.read_parquet(os.path.join(project_dir, 'data', 'interim', features_file))\n",
    "        # TODO: Tirar isso daqui\n",
    "        self.features_table['ano_mes_cruzamento'] = pd.to_datetime(self.features_table['ano_mes_cruzamento'], format='%Y-%m')\n",
    "\n",
    "        self.target_table = pd.read_parquet(os.path.join(project_dir, 'data', 'interim', target_file))\n",
    "        self.mapping = json.load(open(os.path.join(project_dir, 'data', 'interim', mapping_file), 'r'), \n",
    "                                    object_hook=lambda x: {int(k) : v for k,v in x.items()})\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_table.shape[0]\n",
    "\n",
    "\n",
    "    def _get_data(self, idx):\n",
    "        df = self.target_table.iloc[idx : idx+1].reset_index(drop=True)\n",
    "\n",
    "        cc_num = df['cc_num'][0]\n",
    "        ano_mes_sup = pd.to_datetime(df['ano_mes'][0])\n",
    "        ano_mes_inf = pd.to_datetime(df['ano_mes'][0]) - timedelta(days=366)\n",
    "        range_dates = pd.date_range(ano_mes_inf, ano_mes_sup, inclusive='left', freq='MS')\n",
    "\n",
    "        df_default = pd.DataFrame({\n",
    "            'cc_num' : [cc_num] * 12,\n",
    "            'ano_mes_cruzamento' : range_dates\n",
    "        })\n",
    "\n",
    "        df = df.merge(df_default, how='left', on='cc_num')\n",
    "\n",
    "        df_features = self.features_table.iloc[min(self.mapping[cc_num]): max(self.mapping[cc_num])]\n",
    "        \n",
    "        df = df.merge(df_features, how='left', on=['cc_num', 'ano_mes_cruzamento']).fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        df = self._get_data(idx)\n",
    "\n",
    "        return torch.tensor(df.drop(columns=['ano_mes', 'ano_mes_cruzamento', 'is_fraud', 'cc_num', \n",
    "                                'categorytop1', 'categorytop2', 'categorytop3']).to_numpy().reshape(12,44)).float(), torch.tensor(df['is_fraud'].iloc[0]).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = LSTMDataSet(features_file='features.parquet.gzip', target_file='abt_train.parquet.gzip', mapping_file='dict_cpf_noup.json')\n",
    "test_data = LSTMDataSet(features_file='features.parquet.gzip', target_file='abt_test.parquet.gzip', mapping_file='dict_cpf_noup.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.4 ms, sys: 212 Âµs, total: 32.6 ms\n",
      "Wall time: 30 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [5.6000e+01, 2.4105e+03, 2.0415e+02, 1.8400e+00, 2.0229e+02, 1.0493e+02,\n",
       "          6.4490e+01, 2.1685e+01, 6.9375e+00, 3.4700e+00, 1.8730e+00, 1.0000e+01,\n",
       "          2.6623e+02, 1.0467e+02, 3.1700e+00, 9.8830e+01, 4.6269e+01, 3.3490e+01,\n",
       "          2.0715e+01, 5.0250e+00, 4.4210e+00, 3.2951e+00, 7.0000e+00, 3.0463e+02,\n",
       "          1.1711e+02, 4.8400e+00, 1.1384e+02, 8.4416e+01, 5.7545e+01, 2.7180e+01,\n",
       "          2.0205e+01, 1.4050e+01, 5.7610e+00, 6.0000e+00, 3.5631e+02, 8.2080e+01,\n",
       "          4.2700e+01, 8.1228e+01, 7.3555e+01, 6.4850e+01, 5.8625e+01, 5.0173e+01,\n",
       "          4.5975e+01, 4.3028e+01],\n",
       "         [5.9000e+01, 2.8615e+03, 2.2475e+02, 1.2400e+00, 2.1088e+02, 1.1844e+02,\n",
       "          7.4610e+01, 2.9220e+01, 1.0050e+01, 4.5920e+00, 1.4198e+00, 7.0000e+00,\n",
       "          2.5892e+02, 9.0600e+01, 7.2700e+00, 8.9432e+01, 7.8918e+01, 6.0715e+01,\n",
       "          1.5320e+01, 1.2150e+01, 9.2680e+00, 7.4698e+00, 6.0000e+00, 3.8824e+02,\n",
       "          8.4420e+01, 3.2970e+01, 8.4129e+01, 8.1505e+01, 7.6152e+01, 6.7885e+01,\n",
       "          5.9100e+01, 4.4730e+01, 3.4146e+01, 6.0000e+00, 5.1419e+02, 2.0083e+02,\n",
       "          1.2390e+01, 1.9870e+02, 1.7949e+02, 1.4037e+02, 6.5145e+01, 2.0218e+01,\n",
       "          1.2465e+01, 1.2398e+01],\n",
       "         [8.6000e+01, 6.6727e+03, 8.5281e+02, 1.0700e+00, 8.3347e+02, 1.3355e+02,\n",
       "          8.2402e+01, 4.8865e+01, 1.0100e+01, 4.2550e+00, 1.5375e+00, 2.0000e+01,\n",
       "          1.1550e+03, 1.0350e+02, 1.1740e+01, 1.0067e+02, 8.3671e+01, 6.9290e+01,\n",
       "          5.4910e+01, 4.8333e+01, 3.3798e+01, 1.2012e+01, 9.0000e+00, 8.9936e+02,\n",
       "          1.5446e+02, 7.7540e+01, 1.5088e+02, 1.1862e+02, 1.0683e+02, 9.8230e+01,\n",
       "          8.3310e+01, 7.7828e+01, 7.7569e+01, 8.0000e+00, 5.9853e+02, 2.6179e+02,\n",
       "          1.0700e+00, 2.5101e+02, 1.5400e+02, 8.9980e+01, 5.3735e+01, 2.2830e+01,\n",
       "          6.5370e+00, 1.6167e+00],\n",
       "         [1.0200e+02, 6.4309e+03, 6.9809e+02, 1.1600e+00, 4.7056e+02, 1.1746e+02,\n",
       "          7.3395e+01, 4.3280e+01, 8.9100e+00, 4.5170e+00, 1.4201e+00, 1.9000e+01,\n",
       "          1.0793e+03, 7.6550e+01, 3.9050e+01, 7.6154e+01, 7.1294e+01, 6.1735e+01,\n",
       "          5.7110e+01, 4.9935e+01, 4.4738e+01, 3.9374e+01, 1.5000e+01, 9.5642e+02,\n",
       "          4.7217e+02, 3.1200e+00, 4.4966e+02, 2.2542e+02, 9.6250e+00, 6.4300e+00,\n",
       "          5.3200e+00, 4.0800e+00, 3.2460e+00, 9.0000e+00, 5.4075e+02, 1.6317e+02,\n",
       "          4.2100e+00, 1.5860e+02, 1.1747e+02, 1.0469e+02, 4.8060e+01, 1.3530e+01,\n",
       "          4.4100e+00, 4.2300e+00],\n",
       "         [8.1000e+01, 3.5434e+03, 2.2548e+02, 1.2200e+00, 2.2394e+02, 1.1283e+02,\n",
       "          6.3260e+01, 3.4230e+01, 6.1000e+00, 3.9300e+00, 1.3080e+00, 1.5000e+01,\n",
       "          7.9717e+02, 7.3260e+01, 2.6350e+01, 7.3253e+01, 7.0226e+01, 6.4295e+01,\n",
       "          5.3940e+01, 4.6635e+01, 3.3138e+01, 2.7198e+01, 1.1000e+01, 1.7485e+02,\n",
       "          1.1673e+02, 1.2200e+00, 1.0602e+02, 9.6000e+00, 8.0200e+00, 6.4600e+00,\n",
       "          4.8700e+00, 2.4400e+00, 1.3420e+00, 7.0000e+00, 2.3378e+02, 1.2906e+02,\n",
       "          3.3500e+00, 1.2361e+02, 7.4574e+01, 3.3080e+01, 1.4180e+01, 1.0515e+01,\n",
       "          6.4760e+00, 3.6626e+00],\n",
       "         [1.0900e+02, 8.9938e+03, 3.0751e+03, 1.0200e+00, 1.9613e+02, 1.2127e+02,\n",
       "          8.3580e+01, 5.4050e+01, 9.5200e+00, 5.3900e+00, 1.2720e+00, 1.6000e+01,\n",
       "          1.6314e+03, 1.7047e+02, 7.0140e+01, 1.6524e+02, 1.3480e+02, 1.1632e+02,\n",
       "          9.2945e+01, 8.2522e+01, 7.2180e+01, 7.0271e+01, 1.5000e+01, 9.3079e+02,\n",
       "          9.2550e+01, 4.1220e+01, 9.1216e+01, 8.0504e+01, 7.2910e+01, 5.9960e+01,\n",
       "          5.1470e+01, 4.7944e+01, 4.2144e+01, 1.2000e+01, 3.4127e+03, 3.0751e+03,\n",
       "          1.0200e+00, 2.7534e+03, 1.4493e+02, 6.9668e+01, 5.8300e+00, 2.8825e+00,\n",
       "          1.4200e+00, 1.0464e+00],\n",
       "         [1.0200e+02, 6.3553e+03, 1.2126e+03, 1.1200e+00, 3.9534e+02, 1.0391e+02,\n",
       "          7.4425e+01, 4.2885e+01, 8.4300e+00, 3.8290e+00, 1.3429e+00, 1.3000e+01,\n",
       "          7.4635e+02, 7.6260e+01, 3.3590e+01, 7.6256e+01, 7.5422e+01, 6.5970e+01,\n",
       "          5.7390e+01, 4.8790e+01, 4.1172e+01, 3.4278e+01, 1.2000e+01, 1.2363e+03,\n",
       "          1.4758e+02, 6.3280e+01, 1.4717e+02, 1.4154e+02, 1.0474e+02, 9.7265e+01,\n",
       "          9.5732e+01, 8.2099e+01, 6.5186e+01, 1.2000e+01, 6.2559e+02, 1.3259e+02,\n",
       "          2.2500e+00, 1.3053e+02, 1.1293e+02, 9.8715e+01, 3.1980e+01, 1.9155e+01,\n",
       "          3.9960e+00, 2.3985e+00],\n",
       "         [1.0000e+02, 4.6043e+03, 1.5283e+02, 1.1200e+00, 1.5138e+02, 1.1034e+02,\n",
       "          7.4558e+01, 3.8175e+01, 7.7325e+00, 3.0650e+00, 1.6051e+00, 1.7000e+01,\n",
       "          1.0455e+03, 1.1013e+02, 3.7820e+01, 1.0792e+02, 9.1336e+01, 6.7310e+01,\n",
       "          5.3010e+01, 4.8060e+01, 3.9682e+01, 3.7934e+01, 1.2000e+01, 1.2551e+03,\n",
       "          1.3943e+02, 6.0940e+01, 1.3865e+02, 1.3096e+02, 1.1779e+02, 1.0692e+02,\n",
       "          9.3387e+01, 8.1668e+01, 6.3153e+01, 1.2000e+01, 6.4440e+01, 8.9800e+00,\n",
       "          2.2400e+00, 8.9294e+00, 8.4920e+00, 7.9175e+00, 4.5000e+00, 3.4025e+00,\n",
       "          2.5070e+00, 2.2631e+00],\n",
       "         [8.6000e+01, 4.5858e+03, 4.5809e+02, 1.2700e+00, 2.6276e+02, 1.1306e+02,\n",
       "          6.8033e+01, 3.9885e+01, 9.9775e+00, 4.3100e+00, 1.4570e+00, 1.4000e+01,\n",
       "          5.4306e+02, 1.2242e+02, 2.1100e+00, 1.1528e+02, 6.7365e+01, 5.1232e+01,\n",
       "          3.4365e+01, 1.7350e+01, 8.5410e+00, 2.5455e+00, 1.3000e+01, 7.7067e+02,\n",
       "          7.3870e+01, 4.7070e+01, 7.3763e+01, 7.2128e+01, 6.5950e+01, 5.7710e+01,\n",
       "          5.0350e+01, 4.7224e+01, 4.7083e+01, 1.1000e+01, 6.9545e+02, 2.2829e+02,\n",
       "          9.7500e+00, 2.1823e+02, 1.2769e+02, 8.7785e+01, 3.0620e+01, 1.5000e+01,\n",
       "          1.0660e+01, 9.8410e+00],\n",
       "         [7.9000e+01, 3.1649e+03, 2.1893e+02, 1.0600e+00, 2.1767e+02, 1.0430e+02,\n",
       "          6.2985e+01, 1.5770e+01, 5.7250e+00, 1.9260e+00, 1.1848e+00, 1.0000e+01,\n",
       "          2.7586e+02, 1.3828e+02, 1.6300e+00, 1.2918e+02, 4.7254e+01, 2.6993e+01,\n",
       "          1.4395e+01, 7.0050e+00, 1.8100e+00, 1.6480e+00, 9.0000e+00, 9.8221e+02,\n",
       "          1.6288e+02, 8.6680e+01, 1.5950e+02, 1.2906e+02, 1.1187e+02, 1.0384e+02,\n",
       "          9.8670e+01, 8.8280e+01, 8.6840e+01, 9.0000e+00, 3.2055e+02, 2.1893e+02,\n",
       "          1.2200e+00, 2.0778e+02, 1.0739e+02, 8.3800e+00, 2.4700e+00, 1.5200e+00,\n",
       "          1.4520e+00, 1.2432e+00]]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "training_data.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 12, 44]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for _, batch in enumerate(train_loader):\n",
    "    x_batch, y_batch = batch[0], batch[1]\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(44, 256, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return self.sig(out)\n",
    "\n",
    "model = LSTM(44, 256, 3)\n",
    "model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train(True)\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        x_batch, y_batch = batch[0], batch[1]\n",
    "\n",
    "        output = model(x_batch)[:,1]\n",
    "        loss = loss_function(output, y_batch)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 9:  # print every 10 batches\n",
    "            avg_loss_across_batches = running_loss / 10\n",
    "            print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1,\n",
    "                                                    avg_loss_across_batches))\n",
    "            running_loss = 0.0\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Batch 10, Loss: 0.144\n",
      "Batch 20, Loss: 0.205\n",
      "Batch 30, Loss: 0.220\n",
      "Batch 40, Loss: 0.181\n",
      "Batch 50, Loss: 0.103\n",
      "Batch 60, Loss: 0.245\n",
      "Batch 70, Loss: 0.202\n",
      "Batch 80, Loss: 0.218\n",
      "Batch 90, Loss: 0.127\n",
      "Batch 100, Loss: 0.223\n",
      "Batch 110, Loss: 0.142\n",
      "Batch 120, Loss: 0.240\n",
      "Batch 130, Loss: 0.162\n",
      "Batch 140, Loss: 0.161\n",
      "Batch 150, Loss: 0.149\n",
      "Batch 160, Loss: 0.310\n",
      "Batch 170, Loss: 0.135\n",
      "Batch 180, Loss: 0.127\n",
      "Batch 190, Loss: 0.244\n",
      "Batch 200, Loss: 0.142\n",
      "Batch 210, Loss: 0.119\n",
      "\n",
      "Epoch: 2\n",
      "Batch 10, Loss: 0.331\n",
      "Batch 20, Loss: 0.111\n",
      "Batch 30, Loss: 0.181\n",
      "Batch 40, Loss: 0.121\n",
      "Batch 50, Loss: 0.159\n",
      "Batch 60, Loss: 0.205\n",
      "Batch 70, Loss: 0.232\n",
      "Batch 80, Loss: 0.147\n",
      "Batch 90, Loss: 0.103\n",
      "Batch 100, Loss: 0.181\n",
      "Batch 110, Loss: 0.205\n",
      "Batch 120, Loss: 0.204\n",
      "Batch 130, Loss: 0.165\n",
      "Batch 140, Loss: 0.162\n",
      "Batch 150, Loss: 0.200\n",
      "Batch 160, Loss: 0.239\n",
      "Batch 170, Loss: 0.144\n",
      "Batch 180, Loss: 0.144\n",
      "Batch 190, Loss: 0.140\n",
      "Batch 200, Loss: 0.182\n",
      "Batch 210, Loss: 0.098\n",
      "\n",
      "Epoch: 3\n",
      "Batch 10, Loss: 0.253\n",
      "Batch 20, Loss: 0.184\n",
      "Batch 30, Loss: 0.078\n",
      "Batch 40, Loss: 0.241\n",
      "Batch 50, Loss: 0.099\n",
      "Batch 60, Loss: 0.118\n",
      "Batch 70, Loss: 0.228\n",
      "Batch 80, Loss: 0.162\n",
      "Batch 90, Loss: 0.201\n",
      "Batch 100, Loss: 0.121\n",
      "Batch 110, Loss: 0.220\n",
      "Batch 120, Loss: 0.221\n",
      "Batch 130, Loss: 0.162\n",
      "Batch 140, Loss: 0.142\n",
      "Batch 150, Loss: 0.161\n",
      "Batch 160, Loss: 0.220\n",
      "Batch 170, Loss: 0.237\n",
      "Batch 180, Loss: 0.146\n",
      "Batch 190, Loss: 0.140\n",
      "Batch 200, Loss: 0.163\n",
      "Batch 210, Loss: 0.202\n",
      "\n",
      "Epoch: 4\n",
      "Batch 10, Loss: 0.063\n",
      "Batch 20, Loss: 0.203\n",
      "Batch 30, Loss: 0.141\n",
      "Batch 40, Loss: 0.235\n",
      "Batch 50, Loss: 0.143\n",
      "Batch 60, Loss: 0.258\n",
      "Batch 70, Loss: 0.165\n",
      "Batch 80, Loss: 0.163\n",
      "Batch 90, Loss: 0.200\n",
      "Batch 100, Loss: 0.181\n",
      "Batch 110, Loss: 0.142\n",
      "Batch 120, Loss: 0.161\n",
      "Batch 130, Loss: 0.161\n",
      "Batch 140, Loss: 0.180\n",
      "Batch 150, Loss: 0.179\n",
      "Batch 160, Loss: 0.162\n",
      "Batch 170, Loss: 0.238\n",
      "Batch 180, Loss: 0.105\n",
      "Batch 190, Loss: 0.180\n",
      "Batch 200, Loss: 0.201\n",
      "Batch 210, Loss: 0.313\n",
      "\n",
      "Epoch: 5\n",
      "Batch 10, Loss: 0.181\n",
      "Batch 20, Loss: 0.123\n",
      "Batch 30, Loss: 0.204\n",
      "Batch 40, Loss: 0.140\n",
      "Batch 50, Loss: 0.140\n",
      "Batch 60, Loss: 0.181\n",
      "Batch 70, Loss: 0.098\n",
      "Batch 80, Loss: 0.161\n",
      "Batch 90, Loss: 0.306\n",
      "Batch 100, Loss: 0.161\n",
      "Batch 110, Loss: 0.235\n",
      "Batch 120, Loss: 0.091\n",
      "Batch 130, Loss: 0.277\n",
      "Batch 140, Loss: 0.180\n",
      "Batch 150, Loss: 0.122\n",
      "Batch 160, Loss: 0.178\n",
      "Batch 170, Loss: 0.181\n",
      "Batch 180, Loss: 0.142\n",
      "Batch 190, Loss: 0.142\n",
      "Batch 200, Loss: 0.340\n",
      "Batch 210, Loss: 0.182\n",
      "\n",
      "Epoch: 6\n",
      "Batch 10, Loss: 0.146\n",
      "Batch 20, Loss: 0.181\n",
      "Batch 30, Loss: 0.121\n",
      "Batch 40, Loss: 0.139\n",
      "Batch 50, Loss: 0.306\n",
      "Batch 60, Loss: 0.197\n",
      "Batch 70, Loss: 0.199\n",
      "Batch 80, Loss: 0.145\n",
      "Batch 90, Loss: 0.217\n",
      "Batch 100, Loss: 0.181\n",
      "Batch 110, Loss: 0.161\n",
      "Batch 120, Loss: 0.181\n",
      "Batch 130, Loss: 0.196\n",
      "Batch 140, Loss: 0.179\n",
      "Batch 150, Loss: 0.220\n",
      "Batch 160, Loss: 0.238\n",
      "Batch 170, Loss: 0.147\n",
      "Batch 180, Loss: 0.106\n",
      "Batch 190, Loss: 0.160\n",
      "Batch 200, Loss: 0.182\n",
      "Batch 210, Loss: 0.098\n",
      "\n",
      "Epoch: 7\n",
      "Batch 10, Loss: 0.204\n",
      "Batch 20, Loss: 0.202\n",
      "Batch 30, Loss: 0.218\n",
      "Batch 40, Loss: 0.181\n",
      "Batch 50, Loss: 0.199\n",
      "Batch 60, Loss: 0.234\n",
      "Batch 70, Loss: 0.182\n",
      "Batch 80, Loss: 0.199\n",
      "Batch 90, Loss: 0.216\n",
      "Batch 100, Loss: 0.125\n",
      "Batch 110, Loss: 0.161\n",
      "Batch 120, Loss: 0.180\n",
      "Batch 130, Loss: 0.139\n",
      "Batch 140, Loss: 0.118\n",
      "Batch 150, Loss: 0.207\n",
      "Batch 160, Loss: 0.182\n",
      "Batch 170, Loss: 0.121\n",
      "Batch 180, Loss: 0.119\n",
      "Batch 190, Loss: 0.160\n",
      "Batch 200, Loss: 0.264\n",
      "Batch 210, Loss: 0.102\n",
      "\n",
      "Epoch: 8\n",
      "Batch 10, Loss: 0.255\n",
      "Batch 20, Loss: 0.162\n",
      "Batch 30, Loss: 0.199\n",
      "Batch 40, Loss: 0.238\n",
      "Batch 50, Loss: 0.183\n",
      "Batch 60, Loss: 0.049\n",
      "Batch 70, Loss: 0.078\n",
      "Batch 80, Loss: 0.163\n",
      "Batch 90, Loss: 0.141\n",
      "Batch 100, Loss: 0.206\n",
      "Batch 110, Loss: 0.182\n",
      "Batch 120, Loss: 0.218\n",
      "Batch 130, Loss: 0.198\n",
      "Batch 140, Loss: 0.126\n",
      "Batch 150, Loss: 0.103\n",
      "Batch 160, Loss: 0.285\n",
      "Batch 170, Loss: 0.294\n",
      "Batch 180, Loss: 0.219\n",
      "Batch 190, Loss: 0.165\n",
      "Batch 200, Loss: 0.130\n",
      "Batch 210, Loss: 0.141\n",
      "\n",
      "Epoch: 9\n",
      "Batch 10, Loss: 0.202\n",
      "Batch 20, Loss: 0.221\n",
      "Batch 30, Loss: 0.180\n",
      "Batch 40, Loss: 0.199\n",
      "Batch 50, Loss: 0.181\n",
      "Batch 60, Loss: 0.216\n",
      "Batch 70, Loss: 0.220\n",
      "Batch 80, Loss: 0.199\n",
      "Batch 90, Loss: 0.181\n",
      "Batch 100, Loss: 0.181\n",
      "Batch 110, Loss: 0.200\n",
      "Batch 120, Loss: 0.160\n",
      "Batch 130, Loss: 0.121\n",
      "Batch 140, Loss: 0.078\n",
      "Batch 150, Loss: 0.204\n",
      "Batch 160, Loss: 0.222\n",
      "Batch 170, Loss: 0.219\n",
      "Batch 180, Loss: 0.160\n",
      "Batch 190, Loss: 0.142\n",
      "Batch 200, Loss: 0.122\n",
      "Batch 210, Loss: 0.100\n",
      "\n",
      "Epoch: 10\n",
      "Batch 10, Loss: 0.182\n",
      "Batch 20, Loss: 0.242\n",
      "Batch 30, Loss: 0.160\n",
      "Batch 40, Loss: 0.179\n",
      "Batch 50, Loss: 0.120\n",
      "Batch 60, Loss: 0.138\n",
      "Batch 70, Loss: 0.118\n",
      "Batch 80, Loss: 0.202\n",
      "Batch 90, Loss: 0.161\n",
      "Batch 100, Loss: 0.178\n",
      "Batch 110, Loss: 0.102\n",
      "Batch 120, Loss: 0.217\n",
      "Batch 130, Loss: 0.245\n",
      "Batch 140, Loss: 0.160\n",
      "Batch 150, Loss: 0.163\n",
      "Batch 160, Loss: 0.260\n",
      "Batch 170, Loss: 0.181\n",
      "Batch 180, Loss: 0.144\n",
      "Batch 190, Loss: 0.121\n",
      "Batch 200, Loss: 0.221\n",
      "Batch 210, Loss: 0.217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(False)\n",
    "predict = []\n",
    "y_true = []\n",
    "for batch_index, batch in enumerate(test_loader):\n",
    "    x_batch, y_batch = batch[0], batch[1]\n",
    "    with torch.no_grad():\n",
    "        predict += model(x_batch)[:,1].tolist()\n",
    "        y_true += y_batch.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49748743718592964"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score([int(i) for i in y_true], predict, max_fpr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "roc_curve([int(i) for i in y_true], predict).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
